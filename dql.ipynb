{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Import libraries"
      ],
      "metadata": {
        "id": "OYJd76T5j4_b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import gymnasium as gym\n",
        "import math\n",
        "import random\n",
        "import matplotlib\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import namedtuple, deque\n",
        "from itertools import count\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "\n",
        "from IPython import display\n",
        "\n",
        "env = gym.make(\"CartPole-v1\", render_mode=\"human\")\n",
        "\n",
        "device = \"cpu\"\n",
        "\n",
        "Transition = namedtuple('Transition',\n",
        "                        ('state', 'action', 'next_state', 'reward'))\n",
        "\n"
      ],
      "metadata": {
        "id": "-ANoMUDFj5Ro"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating Replay Memory"
      ],
      "metadata": {
        "id": "wO_2NkAlj45J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReplayMemory(object):\n",
        "\n",
        "      def __init__(self, capacity): # initializer\n",
        "          self.memory = deque([],maxlen=capacity)\n",
        "\n",
        "      def push(self, *args): # save a transition\n",
        "          self.memory.append(Transition(*args))\n",
        "\n",
        "      def sample(self, batch_size): # choose random sample\n",
        "          return random.sample(self.memory, batch_size)\n",
        "\n",
        "      def __len__(self): # memory length\n",
        "          return len(self.memory)\n"
      ],
      "metadata": {
        "id": "Iw3Pvyamj5pT"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Creating a DQL Model"
      ],
      "metadata": {
        "id": "iNAFPyOFj4z8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DQN(nn.Module):\n",
        "    def __init__(self, observations, n_actions):\n",
        "        super(DQN, self).__init__()\n",
        "\n",
        "        self.layer1 = nn.Linear(observations, 128)\n",
        "        self.layer2 = nn.Linear(128, 128)\n",
        "        self.layer3 = nn.Linear(128, n_actions)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = F.relu(self.layer1(x))\n",
        "        x = F.relu(self.layer2(x))\n",
        "        return self.layer3(x)"
      ],
      "metadata": {
        "id": "pRzqxfUtj57Q"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Determination of hyperparameters and auxiliary functions"
      ],
      "metadata": {
        "id": "Qg2MTQLaj4tu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 128\n",
        "gamma = 0.99 # discount factor\n",
        "eps_start = 0.9\n",
        "eps_end = 0.05\n",
        "eps_decay = 1000\n",
        "tau = 0.005 # update rate of target network\n",
        "lr = 1e-4\n",
        "\n",
        "n_actions = env.action_space.n # number of actions\n",
        "state, info = env.reset() # reset environment. state = initial state\n",
        "n_observations = len(state)\n",
        "\n",
        "policy_net = DQN(n_observations, n_actions).to(device)\n",
        "target_net = DQN(n_observations, n_actions).to(device)\n",
        "target_net.load_state_dict(policy_net.state_dict())\n",
        "\n",
        "optimizer = optim.AdamW(policy_net.parameters(), lr=lr, amsgrad=True)\n",
        "memory = ReplayMemory(10000)\n",
        "\n",
        "steps_done = 0\n",
        "\n",
        "def select_action(state):\n",
        "    global steps_done\n",
        "    sample = random.random()\n",
        "    eps_threshold = eps_end + (eps_start - eps_end) * math.exp(-1 * steps_done / eps_decay)\n",
        "    steps_done += 1\n",
        "\n",
        "    # If the sample is greater than eps_threshold, the agent chooses an action with the neural network; otherwise, a random action is chosen.\n",
        "    if sample > eps_threshold:\n",
        "        with torch.no_grad():\n",
        "\n",
        "            return policy_net(state).max(1).indices.view(1, 1)\n",
        "    else:\n",
        "        return torch.tensor([[env.action_space.sample()]], device=device, dtype=torch.long)\n",
        "\n",
        "episode_durations = []\n",
        "\n",
        "def plot_durations(show_result=False):\n",
        "    plt.figure(1)\n",
        "    durations_t = torch.tensor(episode_durations, dtype=torch.float)\n",
        "    if show_result:\n",
        "        plt.title('Result')\n",
        "    else:\n",
        "        plt.clf()\n",
        "        plt.title('Training...')\n",
        "\n",
        "    plt.xlabel('Episode')\n",
        "    plt.ylabel('Duration')\n",
        "    plt.plot(durations_t.numpy())\n",
        "\n",
        "    if len(durations_t) >= 100:\n",
        "        means = durations_t.unfold(0, 100, 1).mean(1).view(-1)\n",
        "        means = torch.cat((torch.zeros(99), means))\n",
        "        plt.plot(means.numpy())\n",
        "\n",
        "    plt.pause(0.001)\n",
        "\n",
        "    display.display(plt.gcf())\n",
        "    display.clear_output(wait=True)\n",
        "\n",
        "def optimize_model():\n",
        "  # Check if there are enough experiences in memory, otherwise exit the function\n",
        "    if len(memory) < batch_size:\n",
        "        return\n",
        "\n",
        "    # a random sample of experiences is taken from memory\n",
        "    transitions = memory.sample(batch_size)\n",
        "\n",
        "    # separation process\n",
        "    batch = Transition(*zip(*transitions))\n",
        "\n",
        "    # Creates a boolean mask whose next states are none\n",
        "    non_final_mask = torch.tensor(tuple(map(lambda s: s is not None, batch.next_state)), device=device, dtype=torch.bool)\n",
        "\n",
        "    # combines all non-terminal states into a single tensor\n",
        "    non_final_next_states = torch.cat([s for s in batch.next_state if s is not None])\n",
        "\n",
        "    # The state, action, and reward within the group are combined\n",
        "    state_batch = torch.cat(batch.state)\n",
        "    action_batch = torch.cat(batch.action)\n",
        "    reward_batch = torch.cat(batch.reward)\n",
        "\n",
        "    state_action_values = policy_net(state_batch).gather(1, action_batch)\n",
        "\n",
        "    next_state_values = torch.zeros(batch_size, device=device)\n",
        "    with torch.no_grad():\n",
        "        next_state_values[non_final_mask] = target_net(non_final_next_states).max(1).values\n",
        "\n",
        "    expected_state_action_values = (next_state_values * gamma) + reward_batch\n",
        "\n",
        "    criterion = nn.SmoothL1Loss()\n",
        "    loss = criterion(state_action_values, expected_state_action_values.unsqueeze(1))\n",
        "\n",
        "    optimizer.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_value_(policy_net.parameters(), 100)\n",
        "    optimizer.step()\n",
        "\n"
      ],
      "metadata": {
        "id": "PCXWsE5Wj6J3"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Model training and evaluation of results"
      ],
      "metadata": {
        "id": "ik0bkBbgj4Y3"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "487DZdH2jKi-",
        "outputId": "ef7238b7-ef57-4be8-bc59-b5464cb9c318"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "<Figure size 640x480 with 0 Axes>"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "num_episodes = 200\n",
        "\n",
        "for i_episode in range(num_episodes):\n",
        "\n",
        "    # reset env\n",
        "    state, info = env.reset()\n",
        "    state = torch.tensor(state, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "    for t in count():\n",
        "        action = select_action(state) # choosing action\n",
        "        observation, reward, terminated, truncated, _ = env.step(action.item())\n",
        "        reward = torch.tensor([reward], device=device)\n",
        "        done = terminated or truncated # we lost\n",
        "\n",
        "        if terminated:\n",
        "            next_state = None\n",
        "        else:\n",
        "            next_state = torch.tensor(observation, dtype=torch.float32, device=device).unsqueeze(0)\n",
        "\n",
        "        # stores transitons in memory\n",
        "        memory.push(state, action, next_state, reward)\n",
        "\n",
        "        #update state\n",
        "        state = next_state\n",
        "\n",
        "        # training\n",
        "        optimize_model()\n",
        "\n",
        "        # update network parameters\n",
        "        target_net_state_dict = target_net.state_dict()\n",
        "        policy_net_state_dict = policy_net.state_dict()\n",
        "\n",
        "        for key in policy_net_state_dict:\n",
        "            target_net_state_dict[key] = policy_net_state_dict[key]*tau + target_net_state_dict[key]*(1-tau)\n",
        "            target_net.load_state_dict(target_net_state_dict)\n",
        "\n",
        "        if done:\n",
        "            episode_durations.append(t + 1)\n",
        "            plot_durations()\n",
        "            break\n",
        "\n",
        "print('Complete')\n",
        "plot_durations(show_result=True)\n",
        "plt.ioff()\n",
        "plt.show()\n",
        "\n"
      ]
    }
  ]
}